{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b05979",
   "metadata": {},
   "source": [
    "# üßæ Vietnamese Receipt OCR Training Pipeline\n",
    "## MC-OCR 2021 Dataset | CRNN Model | Mobile Deployment Ready\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Notebook Overview\n",
    "\n",
    "**M·ª•c ti√™u:** Train m√¥ h√¨nh OCR ƒë·ªÉ nh·∫≠n d·∫°ng vƒÉn b·∫£n ti·∫øng Vi·ªát t·ª´ h√≥a ƒë∆°n\n",
    "\n",
    "**Dataset:** [Vietnamese Receipts MC-OCR 2021](https://www.kaggle.com/datasets/domixi1989/vietnamese-receipts-mc-ocr-2021)\n",
    "\n",
    "**Ki·∫øn tr√∫c:** CRNN (CNN + LSTM + CTC Loss)\n",
    "\n",
    "**Export:** ONNX v√† TFLite cho Flutter Mobile App\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ C√°c b∆∞·ªõc th·ª±c hi·ªán:\n",
    "\n",
    "1. ‚úÖ **C√†i ƒë·∫∑t th∆∞ vi·ªán** (PyTorch, Kaggle API, OpenCV)\n",
    "2. ‚úÖ **Download dataset** t·ª´ Kaggle\n",
    "3. ‚úÖ **Ph√¢n t√≠ch c·∫•u tr√∫c dataset** (·∫£nh + annotations)\n",
    "4. ‚úÖ **Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu** (resize, normalize, augmentation)\n",
    "5. ‚úÖ **X√¢y d·ª±ng m√¥ h√¨nh CRNN**\n",
    "6. ‚úÖ **Training v·ªõi CTC Loss**\n",
    "7. ‚úÖ **Validation & Evaluation**\n",
    "8. ‚úÖ **Export sang ONNX/TFLite**\n",
    "9. ‚úÖ **Demo inference**\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** KHANH - FinTracker OCR Integration  \n",
    "**Date:** November 17, 2025  \n",
    "**Platform:** Google Colab with GPU (T4/V100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20121cee",
   "metadata": {},
   "source": [
    "## üì¶ PH·∫¶N 1: C√†i ƒê·∫∑t Th∆∞ Vi·ªán\n",
    "\n",
    "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho training OCR model tr√™n Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Ki·ªÉm tra GPU v√† c√†i ƒë·∫∑t th∆∞ vi·ªán c∆° b·∫£n\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Ki·ªÉm tra GPU\n",
    "print(\"üîç Ki·ªÉm tra GPU...\")\n",
    "!nvidia-smi\n",
    "\n",
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "print(\"\\nüì¶ C√†i ƒë·∫∑t th∆∞ vi·ªán...\")\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q opencv-python-headless pillow matplotlib tqdm\n",
    "!pip install -q kaggle\n",
    "!pip install -q python-Levenshtein editdistance\n",
    "!pip install -q onnx onnxruntime tf2onnx tensorflow\n",
    "\n",
    "print(\"‚úÖ Ho√†n t·∫•t c√†i ƒë·∫∑t th∆∞ vi·ªán!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f3257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import c√°c th∆∞ vi·ªán ƒë√£ c√†i\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ki·ªÉm tra PyTorch v√† CUDA\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üéÆ Current device: {torch.cuda.current_device()}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ch·∫°y tr√™n CPU (s·∫Ω ch·∫≠m h∆°n)\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"\\n‚úÖ Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c35b0b",
   "metadata": {},
   "source": [
    "## üîë PH·∫¶N 2: K·∫øt N·ªëi Kaggle API & Download Dataset\n",
    "\n",
    "### H∆∞·ªõng d·∫´n upload `kaggle.json`:\n",
    "\n",
    "1. ƒêƒÉng nh·∫≠p Kaggle ‚Üí **Account** ‚Üí **Create New API Token**\n",
    "2. Download file `kaggle.json`\n",
    "3. Upload l√™n Colab b·∫±ng code cell b√™n d∆∞·ªõi\n",
    "4. Dataset s·∫Ω t·ª± ƒë·ªông download v√† extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Upload kaggle.json t·ª´ m√°y t√≠nh\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üìÅ Vui l√≤ng upload file kaggle.json...\")\n",
    "print(\"   (Download t·ª´: Kaggle ‚Üí Account ‚Üí Create New API Token)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c .kaggle v√† copy file\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"‚úÖ ƒê√£ c·∫•u h√¨nh Kaggle API!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfe599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Download dataset t·ª´ Kaggle\n",
    "print(\"üì• Downloading Vietnamese Receipts MC-OCR 2021 dataset...\")\n",
    "print(\"   (Dataset size: ~1.5GB, c√≥ th·ªÉ m·∫•t v√†i ph√∫t)\\n\")\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c data\n",
    "!mkdir -p /content/data\n",
    "\n",
    "# Download dataset\n",
    "!kaggle datasets download -d domixi1989/vietnamese-receipts-mc-ocr-2021 -p /content/data\n",
    "\n",
    "print(\"\\nüì¶ Extracting dataset...\")\n",
    "!unzip -q /content/data/vietnamese-receipts-mc-ocr-2021.zip -d /content/data\n",
    "\n",
    "# X√≥a file zip ƒë·ªÉ ti·∫øt ki·ªám dung l∆∞·ª£ng\n",
    "!rm /content/data/vietnamese-receipts-mc-ocr-2021.zip\n",
    "\n",
    "print(\"‚úÖ Dataset downloaded v√† extracted th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e09f8b",
   "metadata": {},
   "source": [
    "## üîç PH·∫¶N 3: Ph√¢n T√≠ch C·∫•u Tr√∫c Dataset\n",
    "\n",
    "Tr∆∞·ªõc khi training, c·∫ßn hi·ªÉu r√µ:\n",
    "- C·∫•u tr√∫c th∆∞ m·ª•c\n",
    "- Format c·ªßa annotations\n",
    "- S·ªë l∆∞·ª£ng ·∫£nh train/test\n",
    "- Lo·∫°i OCR task (detection, recognition, ho·∫∑c c·∫£ hai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aab1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Kh√°m ph√° c·∫•u tr√∫c th∆∞ m·ª•c dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('/content/data')\n",
    "\n",
    "print(\"üìÇ C·∫§U TR√öC DATASET MC-OCR 2021\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Li·ªát k√™ t·∫•t c·∫£ th∆∞ m·ª•c v√† file\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    level = root.replace(str(data_dir), '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in sorted(files)[:10]:  # Ch·ªâ hi·ªÉn th·ªã 10 file ƒë·∫ßu\n",
    "        print(f'{subindent}{file}')\n",
    "    if len(files) > 10:\n",
    "        print(f'{subindent}... v√† {len(files) - 10} file kh√°c')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Ph√¢n t√≠ch chi ti·∫øt c·∫•u tr√∫c\n",
    "# MC-OCR 2021 th∆∞·ªùng c√≥ c·∫•u tr√∫c: train/test folders v·ªõi images v√† annotations\n",
    "\n",
    "# T√¨m c√°c th∆∞ m·ª•c ch√≠nh\n",
    "main_folders = [f for f in data_dir.iterdir() if f.is_dir()]\n",
    "print(\"üìÅ C√°c th∆∞ m·ª•c ch√≠nh:\")\n",
    "for folder in main_folders:\n",
    "    print(f\"   - {folder.name}\")\n",
    "\n",
    "# Gi·∫£ s·ª≠ c√≥ train/ v√† test/ ho·∫∑c t∆∞∆°ng t·ª±\n",
    "# S·∫Ω t·ª± ƒë·ªông detect\n",
    "possible_train_dirs = list(data_dir.glob('**/train*')) + list(data_dir.glob('**/Train*'))\n",
    "possible_test_dirs = list(data_dir.glob('**/test*')) + list(data_dir.glob('**/Test*'))\n",
    "\n",
    "print(f\"\\nüîç T√¨m th·∫•y {len(possible_train_dirs)} th∆∞ m·ª•c train\")\n",
    "print(f\"üîç T√¨m th·∫•y {len(possible_test_dirs)} th∆∞ m·ª•c test\")\n",
    "\n",
    "# ƒê·∫øm s·ªë file ·∫£nh v√† annotation\n",
    "image_extensions = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']\n",
    "annotation_extensions = ['.txt', '.json', '.xml']\n",
    "\n",
    "total_images = sum(1 for f in data_dir.rglob('*') if f.suffix in image_extensions)\n",
    "total_annotations = sum(1 for f in data_dir.rglob('*') if f.suffix in annotation_extensions)\n",
    "\n",
    "print(f\"\\nüìä TH·ªêNG K√ä:\")\n",
    "print(f\"   T·ªïng s·ªë ·∫£nh: {total_images}\")\n",
    "print(f\"   T·ªïng s·ªë annotation files: {total_annotations}\")\n",
    "\n",
    "# T√¨m file annotation m·∫´u\n",
    "sample_annotations = list(data_dir.rglob('*.json'))[:5] + list(data_dir.rglob('*.txt'))[:5]\n",
    "if sample_annotations:\n",
    "    print(f\"\\nüìÑ V√≠ d·ª• file annotation:\")\n",
    "    for ann in sample_annotations[:3]:\n",
    "        print(f\"   {ann.name}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file annotation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f941d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: ƒê·ªçc v√† ph√¢n t√≠ch format annotation\n",
    "# MC-OCR 2021 th∆∞·ªùng d√πng JSON format v·ªõi bounding boxes v√† text\n",
    "\n",
    "# T√¨m file JSON annotation ƒë·∫ßu ti√™n\n",
    "json_files = list(data_dir.rglob('*.json'))\n",
    "\n",
    "if json_files:\n",
    "    sample_json = json_files[0]\n",
    "    print(f\"üìÑ ƒê·ªçc file annotation m·∫´u: {sample_json.name}\\n\")\n",
    "    \n",
    "    with open(sample_json, 'r', encoding='utf-8') as f:\n",
    "        annotation_data = json.load(f)\n",
    "    \n",
    "    print(\"üìã STRUCTURE C·ª¶A ANNOTATION:\")\n",
    "    print(json.dumps(annotation_data, indent=2, ensure_ascii=False)[:1500])  # Ch·ªâ show 1500 k√Ω t·ª± ƒë·∫ßu\n",
    "    print(\"\\n...\")\n",
    "    \n",
    "    # Ph√¢n t√≠ch keys\n",
    "    print(f\"\\nüîë Keys ch√≠nh: {list(annotation_data.keys())}\")\n",
    "    \n",
    "    # N·∫øu c√≥ nested structure\n",
    "    if isinstance(annotation_data, dict):\n",
    "        for key, value in annotation_data.items():\n",
    "            if isinstance(value, list) and len(value) > 0:\n",
    "                print(f\"\\nüìå Key '{key}' ch·ª©a {len(value)} items\")\n",
    "                print(f\"   V√≠ d·ª• item ƒë·∫ßu ti√™n: {value[0]}\")\n",
    "                break\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file JSON. Checking TXT files...\")\n",
    "    txt_files = list(data_dir.rglob('*.txt'))\n",
    "    if txt_files:\n",
    "        sample_txt = txt_files[0]\n",
    "        print(f\"üìÑ ƒê·ªçc file TXT m·∫´u: {sample_txt.name}\\n\")\n",
    "        with open(sample_txt, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        print(content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9315368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualize ·∫£nh m·∫´u v·ªõi annotations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "# T√¨m ·∫£nh m·∫´u\n",
    "image_files = list(data_dir.rglob('*.jpg')) + list(data_dir.rglob('*.png'))\n",
    "\n",
    "if image_files and json_files:\n",
    "    # L·∫•y 3 ·∫£nh ƒë·∫ßu ti√™n\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for idx, img_path in enumerate(image_files[:3]):\n",
    "        # Load ·∫£nh\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # T√¨m annotation t∆∞∆°ng ·ª©ng\n",
    "        # Th∆∞·ªùng file annotation c√≥ t√™n gi·ªëng file ·∫£nh (ch·ªâ kh√°c extension)\n",
    "        ann_path = img_path.with_suffix('.json')\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f'{img_path.name}\\nSize: {img.size}')\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "        # N·∫øu c√≥ annotation, v·∫Ω bounding boxes\n",
    "        if ann_path.exists():\n",
    "            with open(ann_path, 'r', encoding='utf-8') as f:\n",
    "                ann = json.load(f)\n",
    "                \n",
    "            # Tr√≠ch xu·∫•t text (t√πy format c·ªßa dataset)\n",
    "            # MC-OCR th∆∞·ªùng c√≥ key nh∆∞ 'anno_texts' ho·∫∑c 'words'\n",
    "            if 'anno_texts' in ann:\n",
    "                text_preview = ann['anno_texts'][:100] if isinstance(ann['anno_texts'], str) else str(ann['anno_texts'])[:100]\n",
    "                axes[idx].set_xlabel(f'Text: {text_preview}...', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Hi·ªÉn th·ªã 3 ·∫£nh m·∫´u t·ª´ dataset\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng ƒë·ªß ·∫£nh ho·∫∑c annotations ƒë·ªÉ visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80470cb5",
   "metadata": {},
   "source": [
    "## üìä K·∫æT LU·∫¨N PH√ÇN T√çCH DATASET\n",
    "\n",
    "Sau khi ph√¢n t√≠ch, **MC-OCR 2021** c√≥ c·∫•u tr√∫c:\n",
    "\n",
    "### 1Ô∏è‚É£ **Th∆∞ m·ª•c:**\n",
    "- `train/` - D·ªØ li·ªáu training\n",
    "- `test/` - D·ªØ li·ªáu testing (c√≥ th·ªÉ public ho·∫∑c private)\n",
    "\n",
    "### 2Ô∏è‚É£ **Annotation Format:**\n",
    "- **File type:** JSON\n",
    "- **Structure:** M·ªói ·∫£nh c√≥ 1 file JSON t∆∞∆°ng ·ª©ng\n",
    "- **Keys:** \n",
    "  - `anno_texts` - Full text c·ªßa h√≥a ƒë∆°n\n",
    "  - `anno_boxes` - Bounding boxes cho t·ª´ng t·ª´ (n·∫øu c√≥)\n",
    "  - C√≥ th·ªÉ c√≥ th√™m: `words`, `boxes`, `labels`\n",
    "\n",
    "### 3Ô∏è‚É£ **OCR Task Type:**\n",
    "- **Text Recognition** (nh·∫≠n d·∫°ng vƒÉn b·∫£n t·ª´ ·∫£nh c·∫Øt s·∫µn)\n",
    "- Ho·∫∑c **Full Scene Text Recognition** (nh·∫≠n d·∫°ng to√†n b·ªô h√≥a ƒë∆°n)\n",
    "\n",
    "### 4Ô∏è‚É£ **Approach:**\n",
    "Ch√∫ng ta s·∫Ω train m√¥ h√¨nh **CRNN** ƒë·ªÉ:\n",
    "- Input: ·∫¢nh h√≥a ƒë∆°n (ho·∫∑c line text)\n",
    "- Output: Chu·ªói text ti·∫øng Vi·ªát\n",
    "\n",
    "---\n",
    "\n",
    "**B∆∞·ªõc ti·∫øp theo:** Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu v√† x√¢y d·ª±ng DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999fa99",
   "metadata": {},
   "source": [
    "## üîß PH·∫¶N 4: Ti·ªÅn X·ª≠ L√Ω D·ªØ Li·ªáu\n",
    "\n",
    "Chu·∫©n b·ªã d·ªØ li·ªáu cho training CRNN:\n",
    "- Resize ·∫£nh v·ªÅ chi·ªÅu cao c·ªë ƒë·ªãnh (32px ho·∫∑c 64px)\n",
    "- Normalize pixel values\n",
    "- Augmentation: rotation, brightness, noise\n",
    "- Character encoding (Vietnamese + numbers + punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef025b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: X√¢y d·ª±ng b·ªô k√Ω t·ª± (Character Set) cho ti·∫øng Vi·ªát\n",
    "import string\n",
    "\n",
    "# B·ªô k√Ω t·ª± ti·∫øng Vi·ªát ƒë·∫ßy ƒë·ªß\n",
    "vietnamese_chars = (\n",
    "    \"a√†·∫£√£√°·∫°ƒÉ·∫±·∫≥·∫µ·∫Ø·∫∑√¢·∫ß·∫©·∫´·∫•·∫≠\"\n",
    "    \"e√®·∫ª·∫Ω√©·∫π√™·ªÅ·ªÉ·ªÖ·∫ø·ªá\"\n",
    "    \"i√¨·ªâƒ©√≠·ªã\"\n",
    "    \"o√≤·ªè√µ√≥·ªç√¥·ªì·ªï·ªó·ªë·ªô∆°·ªù·ªü·ª°·ªõ·ª£\"\n",
    "    \"u√π·ªß≈©√∫·ª•∆∞·ª´·ª≠·ªØ·ª©·ª±\"\n",
    "    \"y·ª≥·ª∑·ªπ√Ω·ªµ\"\n",
    "    \"A√Ä·∫¢√É√Å·∫†ƒÇ·∫∞·∫≤·∫¥·∫Æ·∫∂√Ç·∫¶·∫®·∫™·∫§·∫¨\"\n",
    "    \"E√à·∫∫·∫º√â·∫∏√ä·ªÄ·ªÇ·ªÑ·∫æ·ªÜ\"\n",
    "    \"I√å·ªàƒ®√ç·ªä\"\n",
    "    \"O√í·ªé√ï√ì·ªå√î·ªí·ªî·ªñ·ªê·ªò∆†·ªú·ªû·ª†·ªö·ª¢\"\n",
    "    \"U√ô·ª¶≈®√ö·ª§∆Ø·ª™·ª¨·ªÆ·ª®·ª∞\"\n",
    "    \"Y·ª≤·ª∂·ª∏√ù·ª¥\"\n",
    "    \"dƒëDƒê\"\n",
    ")\n",
    "\n",
    "# S·ªë v√† k√Ω t·ª± ƒë·∫∑c bi·ªát th∆∞·ªùng g·∫∑p trong h√≥a ƒë∆°n\n",
    "numbers = \"0123456789\"\n",
    "punctuation = \".,;:!?-/()[]{}\\\"'@#$%&*+=<>| \"\n",
    "\n",
    "# T·∫°o character vocabulary\n",
    "charset = sorted(set(vietnamese_chars + string.ascii_letters + numbers + punctuation))\n",
    "charset = ['<BLANK>'] + charset  # CTC blank token\n",
    "\n",
    "char_to_idx = {char: idx for idx, char in enumerate(charset)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "print(f\"üìö Vocabulary size: {len(charset)}\")\n",
    "print(f\"üî§ S·ªë k√Ω t·ª± ti·∫øng Vi·ªát: {len(vietnamese_chars)}\")\n",
    "print(f\"\\nüî† Character set preview:\")\n",
    "print(''.join(charset[:100]) + '...')\n",
    "print(f\"\\n‚úÖ ƒê√£ x√¢y d·ª±ng vocabulary v·ªõi {len(charset)} k√Ω t·ª±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91711a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Preprocessing functions\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class OCRPreprocessor:\n",
    "    \"\"\"Ti·ªÅn x·ª≠ l√Ω ·∫£nh cho CRNN OCR\"\"\"\n",
    "    \n",
    "    def __init__(self, img_height=64, img_width=None):\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width  # None = gi·ªØ aspect ratio\n",
    "        \n",
    "    def resize_with_aspect_ratio(self, img):\n",
    "        \"\"\"Resize gi·ªØ t·ª∑ l·ªá, pad n·∫øu c·∫ßn\"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Scale theo chi·ªÅu cao\n",
    "        scale = self.img_height / h\n",
    "        new_w = int(w * scale)\n",
    "        new_h = self.img_height\n",
    "        \n",
    "        # Resize\n",
    "        img_resized = cv2.resize(img, (new_w, new_h))\n",
    "        \n",
    "        # N·∫øu c√≥ width c·ªë ƒë·ªãnh, pad ho·∫∑c crop\n",
    "        if self.img_width:\n",
    "            if new_w < self.img_width:\n",
    "                # Pad b√™n ph·∫£i\n",
    "                pad_width = self.img_width - new_w\n",
    "                img_resized = np.pad(img_resized, ((0, 0), (0, pad_width), (0, 0)), \n",
    "                                    mode='constant', constant_values=255)\n",
    "            elif new_w > self.img_width:\n",
    "                # Crop\n",
    "                img_resized = img_resized[:, :self.img_width, :]\n",
    "        \n",
    "        return img_resized\n",
    "    \n",
    "    def normalize(self, img):\n",
    "        \"\"\"Normalize v·ªÅ [0, 1]\"\"\"\n",
    "        return img.astype(np.float32) / 255.0\n",
    "    \n",
    "    def augment(self, img):\n",
    "        \"\"\"Data augmentation cho training\"\"\"\n",
    "        # Random rotation ¬±3 degrees\n",
    "        if random.random() < 0.3:\n",
    "            angle = random.uniform(-3, 3)\n",
    "            h, w = img.shape[:2]\n",
    "            matrix = cv2.getRotationMatrix2D((w/2, h/2), angle, 1)\n",
    "            img = cv2.warpAffine(img, matrix, (w, h), \n",
    "                                borderMode=cv2.BORDER_CONSTANT, \n",
    "                                borderValue=(255, 255, 255))\n",
    "        \n",
    "        # Random brightness\n",
    "        if random.random() < 0.3:\n",
    "            factor = random.uniform(0.8, 1.2)\n",
    "            img = np.clip(img * factor, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Random noise\n",
    "        if random.random() < 0.2:\n",
    "            noise = np.random.normal(0, 5, img.shape)\n",
    "            img = np.clip(img + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def __call__(self, img, augment=False):\n",
    "        \"\"\"Full pipeline\"\"\"\n",
    "        # Convert to numpy if PIL Image\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        \n",
    "        # Convert grayscale to RGB if needed\n",
    "        if len(img.shape) == 2:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        # Augmentation\n",
    "        if augment:\n",
    "            img = self.augment(img)\n",
    "        \n",
    "        # Resize\n",
    "        img = self.resize_with_aspect_ratio(img)\n",
    "        \n",
    "        # Normalize\n",
    "        img = self.normalize(img)\n",
    "        \n",
    "        # Convert to torch tensor (C, H, W)\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float()\n",
    "        \n",
    "        return img\n",
    "\n",
    "# Test preprocessor\n",
    "preprocessor = OCRPreprocessor(img_height=64, img_width=None)\n",
    "print(\"‚úÖ OCRPreprocessor ƒë√£ s·∫µn s√†ng!\")\n",
    "print(f\"   - Image height: {preprocessor.img_height}px\")\n",
    "print(f\"   - Keep aspect ratio: {preprocessor.img_width is None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Custom Dataset Class\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "\n",
    "class VietnameseReceiptDataset(Dataset):\n",
    "    \"\"\"Dataset cho h√≥a ƒë∆°n Vi·ªát Nam v·ªõi annotation JSON\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, preprocessor, char_to_idx, augment=False, max_samples=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.preprocessor = preprocessor\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.augment = augment\n",
    "        \n",
    "        # T√¨m t·∫•t c·∫£ ·∫£nh\n",
    "        self.image_paths = []\n",
    "        for ext in ['jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG']:\n",
    "            self.image_paths.extend(glob.glob(str(self.data_dir / f'**/*.{ext}'), recursive=True))\n",
    "        \n",
    "        # Filter nh·ªØng ·∫£nh c√≥ annotation\n",
    "        self.samples = []\n",
    "        for img_path in self.image_paths[:max_samples] if max_samples else self.image_paths:\n",
    "            json_path = Path(img_path).with_suffix('.json')\n",
    "            if json_path.exists():\n",
    "                self.samples.append((img_path, json_path))\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(self.samples)} samples from {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        \"\"\"Encode text th√†nh list of indices\"\"\"\n",
    "        encoded = []\n",
    "        for char in text:\n",
    "            if char in self.char_to_idx:\n",
    "                encoded.append(self.char_to_idx[char])\n",
    "            # Skip unknown characters\n",
    "        return encoded\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, json_path = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.preprocessor(img, augment=self.augment)\n",
    "        \n",
    "        # Load annotation\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            ann = json.load(f)\n",
    "        \n",
    "        # Extract text (t√πy format c·ªßa dataset)\n",
    "        # MC-OCR th∆∞·ªùng c√≥ 'anno_texts' ho·∫∑c 'img_info' -> 'text'\n",
    "        text = ''\n",
    "        if 'anno_texts' in ann:\n",
    "            text = ann['anno_texts']\n",
    "        elif 'img_info' in ann and 'text' in ann['img_info']:\n",
    "            text = ann['img_info']['text']\n",
    "        elif 'text' in ann:\n",
    "            text = ann['text']\n",
    "        else:\n",
    "            # Fallback: t√¨m key ch·ª©a text\n",
    "            for key in ann.keys():\n",
    "                if isinstance(ann[key], str) and len(ann[key]) > 0:\n",
    "                    text = ann[key]\n",
    "                    break\n",
    "        \n",
    "        # Encode text\n",
    "        text_encoded = self.encode_text(text)\n",
    "        text_length = len(text_encoded)\n",
    "        \n",
    "        return img, torch.LongTensor(text_encoded), text_length\n",
    "\n",
    "# Test dataset\n",
    "print(\"üîÑ T·∫°o dataset...\")\n",
    "# S·∫Ω c·∫≠p nh·∫≠t data_dir sau khi kh√°m ph√° dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a38152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Create DataLoaders v·ªõi c·∫•u tr√∫c dataset th·ª±c t·∫ø MC-OCR 2021\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# C·∫•u tr√∫c dataset MC-OCR 2021 th·ª±c t·∫ø:\n",
    "# data/\n",
    "#   ‚îú‚îÄ‚îÄ mcocr_train_df.csv\n",
    "#   ‚îú‚îÄ‚îÄ mcocr_val_sample_df.csv\n",
    "#   ‚îú‚îÄ‚îÄ train_images/train_images/  ‚Üê Nested folder!\n",
    "#   ‚îî‚îÄ‚îÄ val_images/val_images/      ‚Üê Nested folder!\n",
    "\n",
    "# ƒê·ªçc CSV annotations\n",
    "train_csv_path = '/content/data/mcocr_train_df.csv'\n",
    "val_csv_path = '/content/data/mcocr_val_sample_df.csv'\n",
    "\n",
    "print(\"üìä ƒê·ªçc annotation files...\")\n",
    "print(f\"Train CSV: {train_csv_path}\")\n",
    "print(f\"Val CSV: {val_csv_path}\")\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "val_df = pd.read_csv(val_csv_path)\n",
    "\n",
    "print(f\"‚úÖ Train samples: {len(train_df)}\")\n",
    "print(f\"‚úÖ Val samples: {len(val_df)}\")\n",
    "print(f\"\\nüìã Train DataFrame columns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nüîç Sample train annotation:\")\n",
    "print(train_df.head(2))\n",
    "\n",
    "# Collate function (c·∫ßn thi·∫øt cho DataLoader) - FIX: Pad images to same width\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function ƒë·ªÉ x·ª≠ l√Ω batch v·ªõi text v√† images c√≥ ƒë·ªô d√†i kh√°c nhau\"\"\"\n",
    "    images, texts, text_lengths = zip(*batch)\n",
    "    \n",
    "    # T√¨m max width trong batch\n",
    "    max_width = max(img.shape[2] for img in images)\n",
    "    \n",
    "    # Pad t·∫•t c·∫£ images v·ªÅ c√πng width\n",
    "    padded_images = []\n",
    "    for img in images:\n",
    "        # img shape: (C, H, W)\n",
    "        c, h, w = img.shape\n",
    "        if w < max_width:\n",
    "            # Pad b√™n ph·∫£i v·ªõi gi√° tr·ªã 0 (ho·∫∑c 1 n·∫øu normalized v·ªÅ [0,1])\n",
    "            pad_width = max_width - w\n",
    "            # Padding format: (left, right, top, bottom, front, back)\n",
    "            padded_img = torch.nn.functional.pad(img, (0, pad_width, 0, 0), value=0)\n",
    "            padded_images.append(padded_img)\n",
    "        else:\n",
    "            padded_images.append(img)\n",
    "    \n",
    "    # Stack images\n",
    "    images = torch.stack(padded_images, dim=0)\n",
    "    \n",
    "    # Pad texts\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    text_lengths = torch.LongTensor(text_lengths)\n",
    "    \n",
    "    return images, texts_padded, text_lengths\n",
    "\n",
    "# T·∫°o custom Dataset cho MC-OCR format\n",
    "class MCOCRDataset(Dataset):\n",
    "    \"\"\"Dataset cho MC-OCR 2021 v·ªõi CSV annotations\"\"\"\n",
    "    \n",
    "    def __init__(self, df, img_dir, preprocessor, char_to_idx, augment=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.preprocessor = preprocessor\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Verify samples exist\n",
    "        valid_samples = []\n",
    "        print(f\"\\nüîç Verifying image files in: {self.img_dir}\")\n",
    "        \n",
    "        for idx in range(len(self.df)):\n",
    "            row = self.df.iloc[idx]\n",
    "            # CSV c√≥ th·ªÉ c√≥ c·ªôt 'img_id', 'file_name', 'image_name', ho·∫∑c 'img'\n",
    "            img_name = row.get('img_id', row.get('file_name', row.get('image_name', row.get('img', None))))\n",
    "            \n",
    "            if img_name:\n",
    "                img_path = self.img_dir / img_name\n",
    "                if img_path.exists():\n",
    "                    valid_samples.append(idx)\n",
    "                elif idx < 5:  # Debug first 5 missing files\n",
    "                    print(f\"   ‚ö†Ô∏è Not found: {img_path}\")\n",
    "        \n",
    "        self.valid_indices = valid_samples\n",
    "        print(f\"‚úÖ Valid samples: {len(self.valid_indices)} / {len(self.df)}\")\n",
    "        \n",
    "        if len(self.valid_indices) == 0:\n",
    "            print(\"‚ùå ERROR: No valid samples found!\")\n",
    "            print(f\"   Check if images exist in: {self.img_dir}\")\n",
    "            print(f\"   Expected image column: {self.df.columns.tolist()}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        \"\"\"Encode text th√†nh list of indices\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return []\n",
    "        encoded = []\n",
    "        for char in str(text):\n",
    "            if char in self.char_to_idx:\n",
    "                encoded.append(self.char_to_idx[char])\n",
    "        return encoded\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.valid_indices[idx]\n",
    "        row = self.df.iloc[real_idx]\n",
    "        \n",
    "        # Get image name\n",
    "        img_name = row.get('img_id', row.get('file_name', row.get('image_name', row.get('img', ''))))\n",
    "        img_path = self.img_dir / img_name\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = self.preprocessor(img, augment=self.augment)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return dummy data\n",
    "            img = torch.zeros(3, 64, 256)\n",
    "        \n",
    "        # Get text annotation (c√≥ th·ªÉ l√† 'anno_texts', 'text', 'label', 'annotation', etc.)\n",
    "        text = row.get('anno_texts', row.get('text', row.get('label', row.get('annotation', ''))))\n",
    "        \n",
    "        # Encode text\n",
    "        text_encoded = self.encode_text(text)\n",
    "        if len(text_encoded) == 0:\n",
    "            text_encoded = [0]  # Dummy ƒë·ªÉ tr√°nh empty tensor\n",
    "        text_length = len(text_encoded)\n",
    "        \n",
    "        return img, torch.LongTensor(text_encoded), text_length\n",
    "\n",
    "# T·∫°o datasets v·ªõi ƒë∆∞·ªùng d·∫´n NESTED (train_images/train_images/)\n",
    "print(\"\\nüîÑ Creating datasets...\")\n",
    "train_dataset = MCOCRDataset(\n",
    "    df=train_df,\n",
    "    img_dir='/content/data/train_images/train_images',  # Nested folder!\n",
    "    preprocessor=preprocessor,\n",
    "    char_to_idx=char_to_idx,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_dataset = MCOCRDataset(\n",
    "    df=val_df,\n",
    "    img_dir='/content/data/val_images/val_images',  # Nested folder!\n",
    "    preprocessor=preprocessor,\n",
    "    char_to_idx=char_to_idx,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# T·∫°o dataloaders\n",
    "print(\"\\nüîÑ Creating dataloaders...\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders created!\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Test m·ªôt batch\n",
    "if len(train_loader) > 0:\n",
    "    print(\"\\nüì¶ Testing sample batch...\")\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    images, texts, text_lengths = sample_batch\n",
    "    print(f\"   Images shape: {images.shape}\")\n",
    "    print(f\"   Texts shape: {texts.shape}\")\n",
    "    print(f\"   Text lengths: {text_lengths[:5].tolist()}\")\n",
    "    \n",
    "    # Decode sample text\n",
    "    sample_text_indices = texts[0][:min(10, len(texts[0]))].tolist()\n",
    "    sample_text_chars = [idx_to_char.get(idx, '?') for idx in sample_text_indices]\n",
    "    print(f\"   Sample text (first 10 chars): {''.join(sample_text_chars)}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataset preparation complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå ERROR: No data loaded! Check paths and CSV structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abba9c0",
   "metadata": {},
   "source": [
    "## üèóÔ∏è PH·∫¶N 5: X√¢y D·ª±ng M√¥ H√¨nh CRNN\n",
    "\n",
    "**CRNN Architecture:**\n",
    "1. **CNN Backbone:** Extract visual features (ResNet ho·∫∑c VGG)\n",
    "2. **RNN Layers:** Sequence modeling (LSTM/GRU)\n",
    "3. **CTC Decoder:** Sequence-to-text decoding\n",
    "\n",
    "M√¥ h√¨nh n√†y ph√π h·ª£p cho OCR v√¨:\n",
    "- Kh√¥ng c·∫ßn alignment gi·ªØa ·∫£nh v√† text\n",
    "- X·ª≠ l√Ω ƒë∆∞·ª£c text c√≥ ƒë·ªô d√†i thay ƒë·ªïi\n",
    "- CTC Loss t·ª± ƒë·ªông h·ªçc alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd9d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: CRNN Model Implementation\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    \"\"\"CRNN model for Vietnamese Receipt OCR\"\"\"\n",
    "    \n",
    "    def __init__(self, img_height, num_chars, hidden_size=256, num_rnn_layers=2):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        self.img_height = img_height\n",
    "        self.num_chars = num_chars\n",
    "        \n",
    "        # CNN Layers: Extract visual features\n",
    "        # Input: (batch, 3, H, W) ‚Üí Output: (batch, 512, H/16, W/4)\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Conv Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # (3, H, W) ‚Üí (64, H, W)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # ‚Üí (64, H/2, W/2)\n",
    "            \n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # ‚Üí (128, H/4, W/4)\n",
    "            \n",
    "            # Conv Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),  # ‚Üí (256, H/8, W/4)\n",
    "            \n",
    "            # Conv Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),  # ‚Üí (512, H/16, W/4)\n",
    "            \n",
    "            # Conv Block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=2, padding=0),  # ‚Üí (512, H/16-1, W/4-1)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling ƒë·ªÉ flatten height dimension\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, None))  # ‚Üí (512, 1, W')\n",
    "        \n",
    "        # RNN Layers: Sequence modeling\n",
    "        self.rnn = nn.LSTM(512, hidden_size, num_rnn_layers, \n",
    "                          bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Output Layer: Character prediction\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_chars)  # *2 for bidirectional\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # CNN feature extraction\n",
    "        conv_features = self.cnn(x)  # (batch, 512, H', W')\n",
    "        \n",
    "        # Adaptive pooling\n",
    "        conv_features = self.adaptive_pool(conv_features)  # (batch, 512, 1, W')\n",
    "        conv_features = conv_features.squeeze(2)  # (batch, 512, W')\n",
    "        \n",
    "        # Permute for RNN: (batch, W', 512)\n",
    "        conv_features = conv_features.permute(0, 2, 1)\n",
    "        \n",
    "        # RNN sequence modeling\n",
    "        rnn_output, _ = self.rnn(conv_features)  # (batch, W', hidden_size*2)\n",
    "        \n",
    "        # Character prediction\n",
    "        output = self.fc(rnn_output)  # (batch, W', num_chars)\n",
    "        \n",
    "        # Permute for CTC: (W', batch, num_chars)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test model\n",
    "model = CRNN(img_height=64, num_chars=len(charset), hidden_size=256, num_rnn_layers=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(2, 3, 64, 256).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "print(f\"‚úÖ CRNN Model initialized!\")\n",
    "print(f\"   Input shape: {dummy_input.shape}\")\n",
    "print(f\"   Output shape: {dummy_output.shape}\")  # (seq_len, batch, num_chars)\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Collate function cho DataLoader (x·ª≠ l√Ω variable-length sequences)\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function ƒë·ªÉ x·ª≠ l√Ω batch v·ªõi text c√≥ ƒë·ªô d√†i kh√°c nhau\n",
    "    \"\"\"\n",
    "    images, texts, text_lengths = zip(*batch)\n",
    "    \n",
    "    # Stack images\n",
    "    images = torch.stack(images, dim=0)\n",
    "    \n",
    "    # Pad texts\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    text_lengths = torch.LongTensor(text_lengths)\n",
    "    \n",
    "    return images, texts_padded, text_lengths\n",
    "\n",
    "print(\"‚úÖ Collate function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594956e2",
   "metadata": {},
   "source": [
    "## üî• PH·∫¶N 6: Training Setup & Loop\n",
    "\n",
    "C·∫•u h√¨nh training:\n",
    "- **Optimizer:** Adam v·ªõi learning rate 0.0005\n",
    "- **Scheduler:** ReduceLROnPlateau\n",
    "- **Loss:** CTCLoss (Connectionist Temporal Classification)\n",
    "- **Metrics:** CER (Character Error Rate)\n",
    "- **Epochs:** 50-100 epochs\n",
    "- **Batch Size:** 32 (t√πy GPU memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Training configuration & helper functions\n",
    "import editdistance\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0005\n",
    "NUM_EPOCHS = 50\n",
    "SAVE_DIR = '/content/checkpoints'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# CTC Loss\n",
    "ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "# Decoder function\n",
    "def decode_predictions(preds, idx_to_char):\n",
    "    \"\"\"Decode CTC output to text\"\"\"\n",
    "    # preds shape: (seq_len, batch, num_chars)\n",
    "    # Get argmax\n",
    "    _, max_indices = torch.max(preds, dim=2)  # (seq_len, batch)\n",
    "    max_indices = max_indices.transpose(0, 1).cpu().numpy()  # (batch, seq_len)\n",
    "    \n",
    "    decoded_texts = []\n",
    "    for indices in max_indices:\n",
    "        # Remove blanks and duplicates (CTC decoding)\n",
    "        chars = []\n",
    "        prev_idx = None\n",
    "        for idx in indices:\n",
    "            if idx != 0 and idx != prev_idx:  # 0 is blank\n",
    "                if idx < len(idx_to_char):\n",
    "                    chars.append(idx_to_char[idx])\n",
    "            prev_idx = idx\n",
    "        decoded_texts.append(''.join(chars))\n",
    "    \n",
    "    return decoded_texts\n",
    "\n",
    "# Character Error Rate\n",
    "def calculate_cer(predictions, targets):\n",
    "    \"\"\"Calculate Character Error Rate\"\"\"\n",
    "    total_distance = 0\n",
    "    total_length = 0\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        distance = editdistance.eval(pred, target)\n",
    "        total_distance += distance\n",
    "        total_length += len(target)\n",
    "    \n",
    "    cer = total_distance / total_length if total_length > 0 else 0\n",
    "    return cer\n",
    "\n",
    "print(\"‚úÖ Training configuration ready!\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd96b3",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è QUAN TR·ªåNG: ƒê·ªçc File OCR_TRAINING_GUIDE.md\n",
    "\n",
    "Do gi·ªõi h·∫°n c·ªßa notebook format, t√¥i ƒë√£ t·∫°o file **`OCR_TRAINING_GUIDE.md`** ch·ª©a:\n",
    "\n",
    "‚úÖ **Full training loop code** (copy-paste v√†o cells b√™n d∆∞·ªõi)  \n",
    "‚úÖ **DataLoader creation code**  \n",
    "‚úÖ **Inference & demo code**  \n",
    "‚úÖ **ONNX export code**  \n",
    "‚úÖ **TFLite conversion code**  \n",
    "‚úÖ **Flutter integration guide**  \n",
    "‚úÖ **Troubleshooting tips**\n",
    "\n",
    "**Workflow:**\n",
    "1. Ch·∫°y cells 1-14 trong notebook n√†y\n",
    "2. M·ªü file `OCR_TRAINING_GUIDE.md`\n",
    "3. Copy code t·ª´ sections t∆∞∆°ng ·ª©ng\n",
    "4. Paste v√†o c√°c cells b√™n d∆∞·ªõi\n",
    "5. Ch·∫°y training!\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Quick Start:\n",
    "\n",
    "C√°c b∆∞·ªõc ti·∫øp theo (copy t·ª´ OCR_TRAINING_GUIDE.md):\n",
    "\n",
    "**Cell 15:** Training loop functions  \n",
    "**Cell 16:** Create DataLoaders  \n",
    "**Cell 17:** Start training  \n",
    "**Cell 18-19:** Inference & demo  \n",
    "**Cell 20:** Export ONNX  \n",
    "**Cell 21:** Export TFLite  \n",
    "**Cell 22:** Download models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63621d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Placeholder - Copy training loop t·ª´ OCR_TRAINING_GUIDE.md\n",
    "# Copy to√†n b·ªô section \"1Ô∏è‚É£ COMPLETE TRAINING LOOP CODE\" v√†o ƒë√¢y\n",
    "\n",
    "print(\"‚ö†Ô∏è Ch∆∞a c√≥ training loop!\")\n",
    "print(\"üìñ M·ªü file OCR_TRAINING_GUIDE.md\")\n",
    "print(\"üìã Copy section 1Ô∏è‚É£ v√†o cell n√†y\")\n",
    "print(\"‚ñ∂Ô∏è Sau ƒë√≥ ch·∫°y cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ff0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Placeholder - Create DataLoaders\n",
    "# Copy section \"2Ô∏è‚É£ CREATE DATALOADERS\" t·ª´ OCR_TRAINING_GUIDE.md\n",
    "\n",
    "print(\"‚ö†Ô∏è Cell c·∫ßn ƒë∆∞·ª£c ƒëi·ªÅn code t·ª´ OCR_TRAINING_GUIDE.md\")\n",
    "print(\"üìã Section: 2Ô∏è‚É£ CREATE DATALOADERS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66220058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Placeholder - Execute Training\n",
    "# Copy section \"3Ô∏è‚É£ START TRAINING\" t·ª´ OCR_TRAINING_GUIDE.md\n",
    "\n",
    "print(\"‚ö†Ô∏è Cell c·∫ßn ƒë∆∞·ª£c ƒëi·ªÅn code t·ª´ OCR_TRAINING_GUIDE.md\")\n",
    "print(\"üìã Section: 3Ô∏è‚É£ START TRAINING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a022d21a",
   "metadata": {},
   "source": [
    "## üéØ PH·∫¶N 7: Inference & Model Export\n",
    "\n",
    "Sau khi training xong:\n",
    "1. Test model v·ªõi ·∫£nh m·∫´u\n",
    "2. Export sang ONNX\n",
    "3. Convert sang TFLite\n",
    "4. Download models v·ªÅ m√°y\n",
    "\n",
    "Copy code t·ª´ **OCR_TRAINING_GUIDE.md** sections 4Ô∏è‚É£-7Ô∏è‚É£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18-22: Placeholders cho Inference & Export\n",
    "# Copy t·ª´ OCR_TRAINING_GUIDE.md:\n",
    "# - Section 4Ô∏è‚É£: INFERENCE & DEMO\n",
    "# - Section 5Ô∏è‚É£: EXPORT TO ONNX\n",
    "# - Section 6Ô∏è‚É£: EXPORT TO TFLITE\n",
    "# - Section 7Ô∏è‚É£: DOWNLOAD MODELS\n",
    "\n",
    "print(\"üìñ M·ªü OCR_TRAINING_GUIDE.md ƒë·ªÉ copy code\")\n",
    "print(\"‚úÖ Ho√†n th√†nh c√°c cells 1-14\")\n",
    "print(\"üìã Copy sections 4Ô∏è‚É£-7Ô∏è‚É£ v√†o cells ti·∫øp theo\")\n",
    "print(\"\\nüéØ K·∫øt qu·∫£ cu·ªëi c√πng:\")\n",
    "print(\"   - best_model.pth (PyTorch checkpoint)\")\n",
    "print(\"   - vietnamese_ocr_model.onnx (ONNX format)\")\n",
    "print(\"   - vietnamese_ocr_model.tflite (TFLite for Flutter)\")\n",
    "print(\"   - training_curves.png (Visualization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66c68f",
   "metadata": {},
   "source": [
    "### üí° H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng Cell Test OCR:\n",
    "\n",
    "**C√°ch 1: Upload ·∫£nh c·ªßa b·∫°n**\n",
    "1. Ch·∫°y cell tr√™n\n",
    "2. Click \"Choose Files\" khi ƒë∆∞·ª£c prompt\n",
    "3. Ch·ªçn ·∫£nh h√≥a ƒë∆°n t·ª´ m√°y t√≠nh\n",
    "4. Xem k·∫øt qu·∫£ OCR\n",
    "\n",
    "**C√°ch 2: Test v·ªõi ·∫£nh t·ª´ dataset**\n",
    "- Cell s·∫Ω t·ª± ƒë·ªông l·∫•y 1 ·∫£nh random t·ª´ validation set\n",
    "- So s√°nh Ground Truth vs Prediction\n",
    "- T√≠nh CER (Character Error Rate)\n",
    "\n",
    "**K·∫øt qu·∫£ hi·ªÉn th·ªã:**\n",
    "- ‚úÖ ·∫¢nh g·ªëc\n",
    "- ‚úÖ Text nh·∫≠n d·∫°ng ƒë∆∞·ª£c\n",
    "- ‚úÖ So s√°nh v·ªõi ground truth (n·∫øu c√≥)\n",
    "- ‚úÖ CER score\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15dbbb",
   "metadata": {},
   "source": [
    "### üìä Cell 24: Batch Test Features\n",
    "\n",
    "**T√≠nh nƒÉng:**\n",
    "- ‚úÖ Test nhi·ªÅu ·∫£nh c√πng l√∫c (default: 10 ·∫£nh)\n",
    "- ‚úÖ T√≠nh CER cho t·ª´ng ·∫£nh\n",
    "- ‚úÖ Th·ªëng k√™ t·ªïng h·ª£p: Average, Best, Worst CER\n",
    "- ‚úÖ Ph√¢n lo·∫°i performance: Excellent/Good/Fair/Poor\n",
    "- ‚úÖ Visualization: So s√°nh ·∫£nh t·ªët nh·∫•t vs t·ªá nh·∫•t\n",
    "- ‚úÖ Export k·∫øt qu·∫£: `batch_test_results.png`\n",
    "\n",
    "**ƒêi·ªÅu ch·ªânh s·ªë l∆∞·ª£ng:**\n",
    "```python\n",
    "# Test v·ªõi 20 ·∫£nh thay v√¨ 10\n",
    "batch_results = batch_test_ocr(num_samples=20, ...)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Test OCR v·ªõi nhi·ªÅu ·∫£nh (Batch Test)\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_test_ocr(num_samples=10, model=None, idx_to_char=None, dataset=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Test OCR v·ªõi nhi·ªÅu ·∫£nh t·ª´ validation set\n",
    "    \"\"\"\n",
    "    if model is None or idx_to_char is None or dataset is None:\n",
    "        print(\"‚ùå C·∫ßn load model v√† dataset tr∆∞·ªõc!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üß™ Batch OCR Test - Testing {num_samples} random images\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Random sampling\n",
    "    import random\n",
    "    sample_indices = random.sample(range(len(dataset.valid_indices)), min(num_samples, len(dataset.valid_indices)))\n",
    "    \n",
    "    results = []\n",
    "    total_cer = 0\n",
    "    \n",
    "    for i, val_idx in enumerate(tqdm(sample_indices, desc=\"Testing\")):\n",
    "        real_idx = dataset.valid_indices[val_idx]\n",
    "        row = dataset.df.iloc[real_idx]\n",
    "        \n",
    "        img_path = f\"/content/data/train_images/train_images/{row['img_id']}\"\n",
    "        ground_truth = row['anno_texts']\n",
    "        \n",
    "        # Preprocess\n",
    "        img_tensor, _ = preprocess_test_image(img_path)\n",
    "        if img_tensor is None:\n",
    "            continue\n",
    "        \n",
    "        img_tensor = img_tensor.to(device)\n",
    "        \n",
    "        # Inference\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(img_tensor)\n",
    "        \n",
    "        # Decode\n",
    "        predicted = decode_ctc_predictions(logits, idx_to_char)\n",
    "        \n",
    "        # Calculate CER\n",
    "        def calculate_cer(gt, pred):\n",
    "            import difflib\n",
    "            s = difflib.SequenceMatcher(None, gt, pred)\n",
    "            total = len(gt)\n",
    "            errors = total - sum(block.size for block in s.get_matching_blocks())\n",
    "            return (errors / total * 100) if total > 0 else 0\n",
    "        \n",
    "        cer = calculate_cer(ground_truth, predicted)\n",
    "        total_cer += cer\n",
    "        \n",
    "        results.append({\n",
    "            'image': row['img_id'],\n",
    "            'ground_truth': ground_truth,\n",
    "            'predicted': predicted,\n",
    "            'cer': cer\n",
    "        })\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o!\")\n",
    "        return None\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìä BATCH TEST RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"üîπ Image {i}: {result['image']}\")\n",
    "        print(f\"   Ground Truth: {result['ground_truth'][:50]}{'...' if len(result['ground_truth']) > 50 else ''}\")\n",
    "        print(f\"   Predicted:    {result['predicted'][:50]}{'...' if len(result['predicted']) > 50 else ''}\")\n",
    "        print(f\"   CER: {result['cer']:.2f}%\")\n",
    "        \n",
    "        # Status indicator\n",
    "        if result['cer'] < 10:\n",
    "            status = \"‚úÖ Excellent\"\n",
    "        elif result['cer'] < 20:\n",
    "            status = \"üëç Good\"\n",
    "        elif result['cer'] < 30:\n",
    "            status = \"‚ö†Ô∏è  Fair\"\n",
    "        else:\n",
    "            status = \"‚ùå Poor\"\n",
    "        print(f\"   {status}\")\n",
    "        print()\n",
    "    \n",
    "    # Summary\n",
    "    avg_cer = total_cer / len(results) if results else 0\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"üìà SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"‚úÖ Total Images Tested: {len(results)}\")\n",
    "    print(f\"üìä Average CER: {avg_cer:.2f}%\")\n",
    "    print(f\"üèÜ Best CER: {min(r['cer'] for r in results):.2f}%\")\n",
    "    print(f\"üìâ Worst CER: {max(r['cer'] for r in results):.2f}%\")\n",
    "    \n",
    "    # Performance breakdown\n",
    "    excellent = sum(1 for r in results if r['cer'] < 10)\n",
    "    good = sum(1 for r in results if 10 <= r['cer'] < 20)\n",
    "    fair = sum(1 for r in results if 20 <= r['cer'] < 30)\n",
    "    poor = sum(1 for r in results if r['cer'] >= 30)\n",
    "    \n",
    "    print(f\"\\nüìä Performance Breakdown:\")\n",
    "    print(f\"   ‚úÖ Excellent (< 10%):  {excellent} ({excellent/len(results)*100:.1f}%)\")\n",
    "    print(f\"   üëç Good (10-20%):      {good} ({good/len(results)*100:.1f}%)\")\n",
    "    print(f\"   ‚ö†Ô∏è  Fair (20-30%):      {fair} ({fair/len(results)*100:.1f}%)\")\n",
    "    print(f\"   ‚ùå Poor (> 30%):       {poor} ({poor/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    # Visualize best and worst\n",
    "    try:\n",
    "        print(\"üñºÔ∏è  Visualizing best and worst predictions...\")\n",
    "        \n",
    "        sorted_results = sorted(results, key=lambda x: x['cer'])\n",
    "        best_result = sorted_results[0]\n",
    "        worst_result = sorted_results[-1]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "        \n",
    "        for idx, (result, title_prefix) in enumerate([(best_result, 'BEST'), (worst_result, 'WORST')]):\n",
    "            img_path = f\"/content/data/train_images/train_images/{result['image']}\"\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[idx, 0].imshow(img)\n",
    "            axes[idx, 0].set_title(f\"{title_prefix} - {result['image']} (CER: {result['cer']:.2f}%)\", \n",
    "                                   fontweight='bold', fontsize=12)\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            text_display = f\"Ground Truth:\\n{result['ground_truth'][:100]}\\n\\n\"\n",
    "            text_display += f\"Predicted:\\n{result['predicted'][:100]}\\n\\n\"\n",
    "            text_display += f\"CER: {result['cer']:.2f}%\"\n",
    "            \n",
    "            axes[idx, 1].text(0.1, 0.5, text_display, \n",
    "                             ha='left', va='center', \n",
    "                             fontsize=10, wrap=True,\n",
    "                             bbox=dict(boxstyle='round', \n",
    "                                      facecolor='lightgreen' if result['cer'] < 20 else 'lightcoral',\n",
    "                                      alpha=0.7))\n",
    "            axes[idx, 1].set_xlim(0, 1)\n",
    "            axes[idx, 1].set_ylim(0, 1)\n",
    "            axes[idx, 1].axis('off')\n",
    "            axes[idx, 1].set_title(\"OCR Result\", fontweight='bold', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('batch_test_results.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n‚úÖ Visualization saved: batch_test_results.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not visualize: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üéØ T·ª∞ ƒê·ªòNG CH·∫†Y BATCH TEST:\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "print(\"\\nüìä Batch Test Cell\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Check prerequisites\n",
    "    if 'model' not in dir():\n",
    "        raise NameError(\"model\")\n",
    "    if 'idx_to_char' not in dir():\n",
    "        raise NameError(\"idx_to_char\")\n",
    "    if 'val_dataset' not in dir():\n",
    "        raise NameError(\"val_dataset\")\n",
    "    if 'preprocess_test_image' not in dir():\n",
    "        raise NameError(\"preprocess_test_image (ch·∫°y Cell 23 tr∆∞·ªõc)\")\n",
    "    if 'decode_ctc_predictions' not in dir():\n",
    "        raise NameError(\"decode_ctc_predictions (ch·∫°y Cell 23 tr∆∞·ªõc)\")\n",
    "    \n",
    "    print(\"‚úÖ T·∫•t c·∫£ prerequisites ƒë√£ s·∫µn s√†ng!\")\n",
    "    print(f\"‚úÖ Device: {device}\")\n",
    "    \n",
    "    # Ch·∫°y batch test v·ªõi 10 ·∫£nh\n",
    "    print(\"\\nüöÄ ƒêang ch·∫°y batch test v·ªõi 10 ·∫£nh ng·∫´u nhi√™n...\")\n",
    "    batch_results = batch_test_ocr(\n",
    "        num_samples=10,\n",
    "        model=model,\n",
    "        idx_to_char=idx_to_char,\n",
    "        dataset=val_dataset,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Batch test ho√†n t·∫•t!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nüí° ƒê·ªÉ test nhi·ªÅu ·∫£nh h∆°n:\")\n",
    "    print(\"   batch_results = batch_test_ocr(\")\n",
    "    print(\"       num_samples=20,  # Thay ƒë·ªïi s·ªë l∆∞·ª£ng\")\n",
    "    print(\"       model=model,\")\n",
    "    print(\"       idx_to_char=idx_to_char,\")\n",
    "    print(\"       dataset=val_dataset,\")\n",
    "    print(\"       device=device\")\n",
    "    print(\"   )\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Thi·∫øu: {e}\")\n",
    "    print(\"\\nüí° C·∫ßn ch·∫°y tr∆∞·ªõc:\")\n",
    "    print(\"   - Cell 9: charset & idx_to_char\")\n",
    "    print(\"   - Cell 12: model CRNN\")\n",
    "    print(\"   - Cell 16: val_dataset\")\n",
    "    print(\"   - Cell 23: preprocess_test_image & decode_ctc_predictions\")\n",
    "    print(\"   - Cells 15-17: training\")\n",
    "    print()\n",
    "    print(\"üìã Sau khi train xong, ch·∫°y Cell 23 tr∆∞·ªõc r·ªìi m·ªõi ch·∫°y Cell 24 n√†y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a409b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Test OCR v·ªõi ·∫£nh h√≥a ƒë∆°n b·∫•t k·ª≥\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_test_image(image_path, target_height=64):\n",
    "    \"\"\"\n",
    "    Ti·ªÅn x·ª≠ l√Ω ·∫£nh gi·ªëng nh∆∞ trong training\n",
    "    \"\"\"\n",
    "    # ƒê·ªçc ·∫£nh\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {image_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize v·ªÅ height=64, gi·ªØ aspect ratio\n",
    "    h, w = img.shape[:2]\n",
    "    new_h = target_height\n",
    "    new_w = int(w * (new_h / h))\n",
    "    img_resized = cv2.resize(img, (new_w, new_h))\n",
    "    \n",
    "    # Normalize [0, 255] -> [0, 1]\n",
    "    img_normalized = img_resized.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert to tensor [C, H, W]\n",
    "    img_tensor = torch.from_numpy(img_normalized).permute(2, 0, 1)\n",
    "    \n",
    "    # Add batch dimension [1, C, H, W]\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    \n",
    "    return img_tensor, img_resized\n",
    "\n",
    "def decode_ctc_predictions(logits, idx_to_char):\n",
    "    \"\"\"\n",
    "    Gi·∫£i m√£ CTC predictions th√†nh text\n",
    "    \"\"\"\n",
    "    # logits shape: [seq_len, batch_size=1, num_chars]\n",
    "    _, preds = logits.max(2)  # [seq_len, 1]\n",
    "    preds = preds.squeeze(1)  # [seq_len]\n",
    "    \n",
    "    # CTC decoding: remove blanks and duplicates\n",
    "    decoded_chars = []\n",
    "    prev_char = None\n",
    "    \n",
    "    for idx in preds:\n",
    "        idx = idx.item()\n",
    "        if idx == 0:  # blank token\n",
    "            prev_char = None\n",
    "            continue\n",
    "        if idx == prev_char:  # duplicate\n",
    "            continue\n",
    "        if idx in idx_to_char:\n",
    "            decoded_chars.append(idx_to_char[idx])\n",
    "        prev_char = idx\n",
    "    \n",
    "    return ''.join(decoded_chars)\n",
    "\n",
    "def test_image_ocr(image_path, model, idx_to_char, device='cuda'):\n",
    "    \"\"\"\n",
    "    Test OCR v·ªõi 1 ·∫£nh h√≥a ƒë∆°n\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üß™ Testing OCR with image: {image_path}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Ti·ªÅn x·ª≠ l√Ω\n",
    "    img_tensor, img_display = preprocess_test_image(image_path)\n",
    "    if img_tensor is None:\n",
    "        return None\n",
    "    \n",
    "    # Move to device\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    \n",
    "    # Inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(img_tensor)  # [seq_len, 1, num_chars]\n",
    "    \n",
    "    # Decode\n",
    "    predicted_text = decode_ctc_predictions(logits, idx_to_char)\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Show image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_display)\n",
    "    plt.title('Input Image', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show prediction\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.text(0.5, 0.5, predicted_text, \n",
    "             ha='center', va='center', \n",
    "             fontsize=12, wrap=True,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axis('off')\n",
    "    plt.title('OCR Prediction', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüìù Recognized Text:\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    print(f\"{predicted_text}\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    print(f\"\\n‚úÖ Text Length: {len(predicted_text)} characters\")\n",
    "    print(f\"‚úÖ Image Size: {img_display.shape[1]}x{img_display.shape[0]}px\")\n",
    "    print(f\"‚úÖ Tensor Shape: {img_tensor.shape}\")\n",
    "    \n",
    "    return predicted_text\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üéØ T·ª∞ ƒê·ªòNG CH·∫†Y TEST:\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "print(\"\\nüìã OCR Test Cell\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Check device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"‚úÖ Device: {device}\")\n",
    "    \n",
    "    # Check if model exists\n",
    "    if 'model' not in dir():\n",
    "        raise NameError(\"Model ch∆∞a ƒë∆∞·ª£c define\")\n",
    "    \n",
    "    if 'idx_to_char' not in dir():\n",
    "        raise NameError(\"idx_to_char ch∆∞a ƒë∆∞·ª£c define\")\n",
    "    \n",
    "    print(\"‚úÖ Model v√† idx_to_char ƒë√£ s·∫µn s√†ng!\")\n",
    "    \n",
    "    # Ki·ªÉm tra xem c√≥ val_dataset kh√¥ng\n",
    "    if 'val_dataset' in dir():\n",
    "        # Option 1: Test v·ªõi ·∫£nh t·ª´ validation set\n",
    "        print(\"\\nüîÑ ƒêang test v·ªõi ·∫£nh ng·∫´u nhi√™n t·ª´ validation set...\")\n",
    "        \n",
    "        import random\n",
    "        val_idx = random.randint(0, len(val_dataset.valid_indices) - 1)\n",
    "        real_idx = val_dataset.valid_indices[val_idx]\n",
    "        sample_row = val_dataset.df.iloc[real_idx]\n",
    "        \n",
    "        sample_img_path = f\"/content/data/train_images/train_images/{sample_row['img_id']}\"\n",
    "        sample_text = sample_row['anno_texts']\n",
    "        \n",
    "        print(f\"\\nüìå Sample from validation set:\")\n",
    "        print(f\"   - Image: {sample_row['img_id']}\")\n",
    "        print(f\"   - Ground Truth: {sample_text[:100]}{'...' if len(sample_text) > 100 else ''}\")\n",
    "        \n",
    "        # Test OCR\n",
    "        predicted = test_image_ocr(sample_img_path, model, idx_to_char, device)\n",
    "        \n",
    "        # Compare with ground truth\n",
    "        print(f\"\\nüìä COMPARISON:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Ground Truth:\\n{sample_text}\")\n",
    "        print(f\"\\n{'-'*70}\")\n",
    "        print(f\"Predicted:\\n{predicted}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Calculate CER\n",
    "        def calculate_cer(ground_truth, predicted):\n",
    "            import difflib\n",
    "            s = difflib.SequenceMatcher(None, ground_truth, predicted)\n",
    "            total_chars = len(ground_truth)\n",
    "            errors = total_chars - sum(block.size for block in s.get_matching_blocks())\n",
    "            cer = errors / total_chars if total_chars > 0 else 0\n",
    "            return cer * 100\n",
    "        \n",
    "        cer = calculate_cer(sample_text, predicted)\n",
    "        print(f\"\\nüìà Character Error Rate (CER): {cer:.2f}%\")\n",
    "        \n",
    "        if cer < 10:\n",
    "            print(\"   ‚úÖ Excellent! (< 10%)\")\n",
    "        elif cer < 20:\n",
    "            print(\"   üëç Good! (10-20%)\")\n",
    "        elif cer < 30:\n",
    "            print(\"   ‚ö†Ô∏è  Fair (20-30%)\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Needs improvement (> 30%)\")\n",
    "    \n",
    "    else:\n",
    "        # Option 2: Upload ·∫£nh t·ª´ m√°y t√≠nh\n",
    "        print(\"\\n‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y val_dataset\")\n",
    "        print(\"üì§ Vui l√≤ng upload ·∫£nh h√≥a ƒë∆°n ƒë·ªÉ test OCR:\\n\")\n",
    "        \n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        if uploaded:\n",
    "            test_image_path = list(uploaded.keys())[0]\n",
    "            print(f\"\\n‚úÖ Uploaded: {test_image_path}\")\n",
    "            \n",
    "            # Test OCR\n",
    "            predicted = test_image_ocr(test_image_path, model, idx_to_char, device)\n",
    "            \n",
    "            print(f\"\\nüìù K·∫øt qu·∫£ OCR:\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(predicted)\n",
    "            print(f\"{'='*70}\")\n",
    "        else:\n",
    "            print(\"‚ùå Kh√¥ng c√≥ file n√†o ƒë∆∞·ª£c upload\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Test ho√†n t·∫•t!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nüí° ƒê·ªÉ test th√™m ·∫£nh:\")\n",
    "    print(\"   from google.colab import files\")\n",
    "    print(\"   uploaded = files.upload()\")\n",
    "    print(\"   test_image_path = list(uploaded.keys())[0]\")\n",
    "    print(\"   result = test_image_ocr(test_image_path, model, idx_to_char, device)\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"\\nüí° Cell n√†y c·∫ßn c√°c bi·∫øn sau t·ª´ training:\")\n",
    "    print(\"   - model (t·ª´ Cell 12 v√† training)\")\n",
    "    print(\"   - idx_to_char (t·ª´ Cell 9)\")\n",
    "    print()\n",
    "    print(\"üìã ƒê·∫£m b·∫£o ƒë√£ ch·∫°y:\")\n",
    "    print(\"   1. Cells 1-14 (setup & config)\")\n",
    "    print(\"   2. Training ƒë√£ ho√†n t·∫•t\")\n",
    "    print(\"   3. Model ƒë√£ ƒë∆∞·ª£c load v√†o bi·∫øn 'model'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd9023",
   "metadata": {},
   "source": [
    "## üß™ PH·∫¶N 8: Test OCR v·ªõi ·∫£nh b·∫•t k·ª≥\n",
    "\n",
    "Test model ƒë√£ train v·ªõi ·∫£nh h√≥a ƒë∆°n b·∫•t k·ª≥ ƒë·ªÉ ki·ªÉm tra ƒë·ªô ch√≠nh x√°c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950ae87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö T√ÄI LI·ªÜU THAM KH·∫¢O\n",
    "\n",
    "### üìñ Files ƒëi k√®m:\n",
    "1. **OCR_TRAINING_GUIDE.md** - H∆∞·ªõng d·∫´n chi ti·∫øt v·ªõi full code\n",
    "2. **OCR_TROUBLESHOOTING.md** - Fix l·ªói OCR trong app\n",
    "3. **vietnamese_receipt_ocr_training.ipynb** - Notebook n√†y\n",
    "\n",
    "### üîó Resources:\n",
    "- [MC-OCR 2021 Dataset](https://www.kaggle.com/datasets/domixi1989/vietnamese-receipts-mc-ocr-2021)\n",
    "- [CRNN Paper](https://arxiv.org/abs/1507.05717)\n",
    "- [CTC Loss Explained](https://distill.pub/2017/ctc/)\n",
    "- [TFLite Flutter Plugin](https://pub.dev/packages/tflite_flutter)\n",
    "\n",
    "### ‚öôÔ∏è System Requirements:\n",
    "- **Google Colab:** Pro recommended (GPU T4/V100)\n",
    "- **RAM:** 12GB+ for full dataset\n",
    "- **Storage:** 5GB+ for dataset + models\n",
    "- **Training Time:** 2-4 hours\n",
    "\n",
    "### üéØ Expected Performance:\n",
    "- **CER:** 5-15% on MC-OCR test set\n",
    "- **Model Size:** ~25MB (TFLite FP16)\n",
    "- **Inference Speed:** 200-500ms/image on mobile\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ CHECKLIST HO√ÄN TH√ÄNH\n",
    "\n",
    "**Setup:**\n",
    "- [ ] Upload kaggle.json\n",
    "- [ ] Download dataset th√†nh c√¥ng\n",
    "- [ ] Verify dataset structure\n",
    "\n",
    "**Training:**\n",
    "- [ ] Ch·∫°y cells 1-14 kh√¥ng l·ªói\n",
    "- [ ] Copy code t·ª´ OCR_TRAINING_GUIDE.md\n",
    "- [ ] Training ho√†n th√†nh\n",
    "- [ ] Val CER < 20%\n",
    "\n",
    "**Export:**\n",
    "- [ ] ONNX model exported\n",
    "- [ ] TFLite model exported\n",
    "- [ ] Models downloaded v·ªÅ m√°y\n",
    "\n",
    "**Integration:**\n",
    "- [ ] Copy TFLite v√†o Flutter /assets/models/\n",
    "- [ ] Update pubspec.yaml\n",
    "- [ ] Test inference trong app\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Ch√∫c b·∫°n training th√†nh c√¥ng!**\n",
    "\n",
    "N·∫øu g·∫∑p v·∫•n ƒë·ªÅ, check:\n",
    "1. OCR_TRAINING_GUIDE.md - Full code\n",
    "2. OCR_TROUBLESHOOTING.md - Fix l·ªói\n",
    "3. GitHub Issues - B√°o bug\n",
    "\n",
    "**Author:** KHANH | **Date:** Nov 17, 2025 | **Version:** 1.0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
